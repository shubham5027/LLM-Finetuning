{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Unsloth"
      ],
      "metadata": {
        "id": "ftrqMtKwm4R4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Unsloth (includes a patched, stable HF + PEFT stack)\n",
        "!pip install -U unsloth\n"
      ],
      "metadata": {
        "id": "dUagkxAlmWYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import and Enviroments"
      ],
      "metadata": {
        "id": "zLmX-_1lnMY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "\n"
      ],
      "metadata": {
        "id": "xSPJ2-b8mc15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) Hugging Face Token"
      ],
      "metadata": {
        "id": "dQUWld8jnPDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Needed only for gated models (LLaMA, some Qwen, etc.)\n",
        "# In Colab: set via environment variable or paste directly (not recommended)\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OQhPKYN9mg2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Model and Tokenizer"
      ],
      "metadata": {
        "id": "UH57hRqnnKRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Base instruct model to fine-tune\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL_NAME,\n",
        "\n",
        "    # Maximum context length supported during training\n",
        "    # Higher = more memory usage\n",
        "    max_seq_length = 2048,\n",
        "\n",
        "    # Computation datatype (fp16 is safest on Colab GPUs)\n",
        "    dtype = torch.float16,\n",
        "\n",
        "    # Enable 4-bit quantization (QLoRA)\n",
        "    load_in_4bit = True,\n",
        "\n",
        "    # Hugging Face auth token (if required)\n",
        "    token = HF_TOKEN,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "nIA4OOb4mvca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Add LoRA Adapters"
      ],
      "metadata": {
        "id": "GHOJhbBJnIaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "\n",
        "    # LoRA rank: controls how much the model can adapt\n",
        "    # 8  ‚Üí small change\n",
        "    # 16 ‚Üí balanced (recommended)\n",
        "    # 32 ‚Üí strong but higher overfitting risk\n",
        "    r = 16,\n",
        "\n",
        "    # Scaling factor for LoRA updates\n",
        "    # Usually 2 √ó r\n",
        "    lora_alpha = 32,\n",
        "\n",
        "    # Dropout applied to LoRA layers (regularization)\n",
        "    lora_dropout = 0.05,\n",
        "\n",
        "    # Apply LoRA only to attention projections\n",
        "    # This is the standard and most stable setup\n",
        "    target_modules = [\"q_proj\", \"v_proj\"],\n",
        "\n",
        "    # Do not train bias terms\n",
        "    bias = \"none\",\n",
        "\n",
        "    # Saves memory by recomputing activations during backward pass\n",
        "    use_gradient_checkpointing = True,\n",
        "\n",
        "    # For reproducibility\n",
        "    random_state = 42,\n",
        ")\n"
      ],
      "metadata": {
        "id": "P5OqdFP4nCPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "5oRAxtt0ncLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instruction-tuning dataset\n",
        "# Using a small slice for safe Colab runs\n",
        "dataset = load_dataset(\n",
        "    \"tatsu-lab/alpaca\",\n",
        "    split = \"train[:2000]\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "moKqId92ndaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Formatting"
      ],
      "metadata": {
        "id": "wFPdDSEMnggU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard instruction ‚Üí response format\n",
        "# The model learns to predict the response given the instruction\n",
        "def format_prompt(example):\n",
        "    return (\n",
        "        \"### Instruction:\\n\"\n",
        "        f\"{example['instruction']}\\n\\n\"\n",
        "        \"### Response:\\n\"\n",
        "        f\"{example['output']}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "yvs17u0PnhrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Dataset"
      ],
      "metadata": {
        "id": "LxCnN18ynkc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataset into a single \"text\" field\n",
        "dataset = dataset.map(\n",
        "    lambda x: {\"text\": format_prompt(x)},\n",
        "    remove_columns = dataset.column_names,\n",
        ")\n"
      ],
      "metadata": {
        "id": "2MBn67_gnmnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Arguments"
      ],
      "metadata": {
        "id": "HEP6Ku7ynpPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # Where checkpoints & logs are saved\n",
        "    output_dir = \"./unsloth-output\",\n",
        "\n",
        "    # Batch size per GPU step\n",
        "    per_device_train_batch_size = 2,\n",
        "\n",
        "    # Accumulate gradients to simulate a larger batch\n",
        "    # Effective batch size = batch_size √ó grad_accumulation\n",
        "    gradient_accumulation_steps = 4,\n",
        "\n",
        "    # Learning rate (LoRA typically uses higher LR)\n",
        "    learning_rate = 2e-4,\n",
        "\n",
        "    # Number of passes over the dataset\n",
        "    num_train_epochs = 1,\n",
        "\n",
        "    # Use mixed precision for speed & memory savings\n",
        "    fp16 = True,\n",
        "\n",
        "    # Print loss every N steps\n",
        "    logging_steps = 10,\n",
        "\n",
        "    # Save model at end of each epoch\n",
        "    save_strategy = \"epoch\",\n",
        "\n",
        "    # Disable external loggers\n",
        "    report_to = \"none\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "V6etMw8wnq1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer"
      ],
      "metadata": {
        "id": "3XjA9HHHnuE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "\n",
        "    # Prepared dataset\n",
        "    train_dataset = dataset,\n",
        "\n",
        "    # Field containing the formatted prompt\n",
        "    dataset_text_field = \"text\",\n",
        "\n",
        "    # Maximum sequence length during training\n",
        "    max_seq_length = 512,\n",
        "\n",
        "    args = training_args,\n",
        ")\n"
      ],
      "metadata": {
        "id": "0bPI1dUpnvdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "NGzrYpCBnzM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start LoRA fine-tuning\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "4tlO66han0M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save LoRA Adapters"
      ],
      "metadata": {
        "id": "K1YI8grYn2r7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saves only the trained LoRA adapters (small size)\n",
        "model.save_pretrained(\"unsloth-lora-adapter\")\n",
        "tokenizer.save_pretrained(\"unsloth-lora-adapter\")\n"
      ],
      "metadata": {
        "id": "789cP0gKn5iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference Test"
      ],
      "metadata": {
        "id": "DYxYuBFGn7j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch model to inference-optimized mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "prompt = \"\"\"### Instruction:\n",
        "Explain LoRA in simple terms.\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens = 150,\n",
        "        temperature = 0.7,\n",
        "    )\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "cOzzzywvn9q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRA Paramenters\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5ouRx1yNoTAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# | `r`    | When to use                  |\n",
        "# | ------ | ---------------------------- |\n",
        "# | 4‚Äì8    | Style / formatting           |\n",
        "# | **16** | ‚úÖ General instruction tuning |\n",
        "# | 32     | Domain-heavy adaptation      |\n",
        "# | 64+    | ‚ùå Overkill on Colab          |\n"
      ],
      "metadata": {
        "id": "qwVXY13EoWf0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîπ lora_alpha ‚Äî Scaling\n",
        "\n",
        "# Controls strength of LoRA updates\n",
        "# Rule:\n",
        "# lora_alpha ‚âà 2 √ó r\n",
        "\n",
        "\n",
        "# Examples:\n",
        "\n",
        "# r=8 ‚Üí alpha=1\n",
        "# r=16 ‚Üí alpha=32 ‚úÖ\n",
        "# r=32 ‚Üí alpha=64"
      ],
      "metadata": {
        "id": "YgfDyw42oeJw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîπ lora_dropout\n",
        "\n",
        "# Regularization for LoRA layers\n",
        "\n",
        "# Value\t        Use case\n",
        "# 0.0\t         Very clean data\n",
        "# 0.05\t       ‚úÖ Default\n",
        "# 0.1\t         Noisy data"
      ],
      "metadata": {
        "id": "sCfgThfIohKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîπ target_modules\n",
        "\n",
        "# Where LoRA is injected\n",
        "\n",
        "# Modules\t                   Use\n",
        "# q_proj, v_proj\t       ‚úÖ Standard\n",
        "# + k_proj, o_proj\t     More capacity\n",
        "# All linear layers\t     ‚ùå Risky\n",
        "\n",
        "# üëâ Don‚Äôt change this early"
      ],
      "metadata": {
        "id": "4DbxtnDloqBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2Ô∏è‚É£ Learning Rate (SECOND MOST IMPORTANT)\n",
        "\n",
        "# LoRA tolerates higher LR than full fine-tuning\n",
        "# LR\t       Result\n",
        "# 1e-4\t     Conservative\n",
        "# 2e-4\t     ‚úÖ Best default\n",
        "# 3e-4\t     Fast, risky\n",
        "# 5e-4+\t     Often unstable"
      ],
      "metadata": {
        "id": "cAhEjolfoz2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3Ô∏è‚É£ Batch Size & Gradient Accumulation\n",
        "\n",
        "# Effective batch size matters more than raw batch\n",
        "\n",
        "# Formula: effective_batch = per_device_batch √ó grad_accum\n",
        "\n",
        "\n",
        "# Typical safe values (Colab):\n",
        "\n",
        "# 2 √ó 4 = 8 ‚úÖ\n",
        "# 1 √ó 8 = 8\n",
        "# 2 √ó 8 = 16 (if VRAM allows)\n",
        "\n",
        "# üëâ Keep effective batch 8‚Äì32"
      ],
      "metadata": {
        "id": "NjxCVapMo9mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epochs (OVERFITTING RISK)\n",
        "\n",
        "# Epochs\t     When\n",
        "# 1\t           ‚úÖDefault\n",
        "# 2            Large / diverse data\n",
        "# 3+\t         ‚ùå Usually overfits\n",
        "\n",
        "# üß† If loss ‚Üì but answers worse ‚Üí stop."
      ],
      "metadata": {
        "id": "EvcfZoV9pLDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5Ô∏è‚É£ Sequence Length\n",
        "\n",
        "# Length\t       Use\n",
        "# 256\t          Short Q&A\n",
        "# 512\t          ‚úÖ Most instruction data\n",
        "# 1024+\t        Only if needed\n",
        "\n",
        "# ‚ö†Ô∏è Longer seq = more VRAM + slower training."
      ],
      "metadata": {
        "id": "v3Bx7NOHpXAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6Ô∏è‚É£ Temperature (INFERENCE ONLY)\n",
        "\n",
        "# Value\t       Behavior\n",
        "# 0.2\t         Deterministic\n",
        "# 0.7\t         ‚úÖ Balanced\n",
        "# 1.0+\t       Creative, risky"
      ],
      "metadata": {
        "id": "smne7Yy9pgln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7Ô∏è‚É£ Common Tuning Recipes\n",
        "\n",
        "\n",
        "# ‚úÖ General Instruction Tuning (RECOMMENDED)\n",
        "# r = 16\n",
        "# alpha = 32\n",
        "# lr = 2e-4\n",
        "# epochs = 1\n",
        "# batch = 2\n",
        "# grad_accum = 4\n",
        "# seq_len = 512\n",
        "\n",
        "\n",
        "# Style / Personality Tuning\n",
        "# r = 8\n",
        "# alpha = 16\n",
        "# lr = 1e-4\n",
        "# epochs = 1\n",
        "\n",
        "\n",
        "\n",
        "# Domain Adaptation\n",
        "# r = 32\n",
        "# alpha = 64\n",
        "# lr = 1e-4\n",
        "# epochs = 2\n"
      ],
      "metadata": {
        "id": "nyKdCO3BpqMj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dyZn4Nrgpzcl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}