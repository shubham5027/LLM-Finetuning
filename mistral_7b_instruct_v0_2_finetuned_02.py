# -*- coding: utf-8 -*-
"""Mistral-7B-Instruct-v0.2_finetuned_02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BzL43O0xpJlGBnjqgwmpL6JWSMO9knoj

<!-- 1ï¸âƒ£Base model :

Mistral-7B-Instruct v0.2
Size: 7 billion parameters
Type: Instruct / chat-tuned
Architecture: Decoder-only Transformer

2ï¸âƒ£Technique: QLoRA (Quantized LoRA)

This combines two ideas:

4-bit Quantization
Base model weights are frozen
Stored in 4-bit (NF4)
Saves a LOT of GPU memory
LoRA (Low-Rank Adaptation)
Small trainable matrices injected into attention layers
Only ~0.2â€“0.5% parameters are trained


3ï¸âƒ£ What Exactly Is Being Trained?
âŒ NOT trained

Embeddings

Attention weights

MLP weights

Output head

âœ… Trained

LoRA adapters only

Inserted into:

q_proj (query projection)
v_proj (value projection)


ðŸ‘‰ This is the industry standard choice for LoRA.

4ï¸âƒ£ LoRA Parameters (VERY IMPORTANT)
Parameter	Value	Meaning
r	16	Rank (capacity of adaptation)
lora_alpha	32	Scaling factor
lora_dropout	0.05	Regularization
target_modules	q_proj, v_proj	Where LoRA is applied
bias	none	Biases not trained

ðŸ§  Rule of thumb:

r=8 â†’ small change

r=16 â†’ balanced (we use this)

r=32+ â†’ higher risk of overfitting

5ï¸âƒ£ Dataset Type
Dataset: Alpaca (instruction tuning)

Structure: Instruction â†’ Response


Example:

Instruction: Explain LoRA
Response: LoRA is a parameter-efficient fine-tuning method...


ðŸ‘‰ This trains behavior, not new knowledge.

6ï¸âƒ£ Prompt Format Used (CRITICAL)
### Instruction:
<task>

### Response:
<ideal answer>


Why this format?

Stable across models

Clear separation of task and output

Matches most instruct checkpoints

âš ï¸ Changing format = changing behavior

7ï¸âƒ£ Sequence Length

Max sequence length (training): 512 tokens

Model capacity: 2048 tokens

Why 512?

Faster training

Lower memory

Most Alpaca samples fit

8ï¸âƒ£ Core Training Hyperparameters
Hyperparameter	Value	Reason
Batch size	2	GPU safe
Gradient accumulation	4	Effective batch = 8
Learning rate	2e-4	LoRA standard
Epochs	1	Avoid overfitting
Precision	FP16	Faster, less memory
Optimizer	AdamW (default)	Stable

ðŸ‘‰ Effective batch size = 2 Ã— 4 = 8

9ï¸âƒ£ What Library Handles What?
Component	Library
Model loading	Unsloth
Quantization	bitsandbytes (internally)
LoRA	PEFT (wrapped by Unsloth)
Training loop	TRL SFTTrainer
Dataset	Hugging Face datasets

Unsloth patches and optimizes all of this internally.

ðŸ”Ÿ What Is Saved After Training?

âœ… LoRA adapter weights only

âŒ NOT the full 7B model

Size:

~50â€“200 MB

Reusable:

Can be attached to base model anytime

Can be pushed to HF Hub

ðŸ§  One-Line Summary (Memorize This)

We freeze a 4-bit Mistral-7B model and train tiny LoRA adapters on instruction data to change its behavior efficiently. -->
"""



# Base model (example):

# Mistral-7B-Instruct v0.2
# Size: 7 billion parameters
# Type: Instruct / chat-tuned
# Architecture: Decoder-only Transformer


# Technique: QLoRA (Quantized LoRA)

# This combines two ideas: 4-bit Quantization + LoRA (Low-Rank Adaptation)

# 4-bit Quantization : Base model weights are frozen
#                      Stored in 4-bit (NF4)
#                      Saves a LOT of GPU memory

# LoRA (Low-Rank Adaptation): Small trainable matrices injected into attention layers
#                             Only ~0.2â€“0.5% parameters are trained





!pip install -U unsloth

import os
import torch
from datasets import load_dataset
from unsloth import FastLanguageModel

# Hugging Face token (Colab: set via environment or manually)
# HF_TOKEN ="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"  # optional for gated models

model_name = "unsloth/Llama-3.2-3B-Instruct"
# You can swap this with Qwen, LLaMA, etc.

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = 2048,
    dtype = torch.float16,
    load_in_4bit = True,

)

# Fixed benchmark prompts (NEVER change once locked)
EVAL_PROMPTS = [
    "Explain LoRA in simple terms.",
    "Write a Python function to reverse a string.",
    "What is gradient accumulation and why is it used?",
    "Explain attention in transformers like I'm 12.",
    "Difference between LoRA and QLoRA.",
]


GEN_KWARGS = dict(
    max_new_tokens = 200,
    temperature = 0.7,
    top_p = 0.9,
)



model = FastLanguageModel.get_peft_model(
    model,
    r = 16,                          # LoRA rank
    lora_alpha = 32,                 # Scaling
    lora_dropout = 0.05,             # Regularization
    target_modules = ["q_proj", "v_proj"],
    bias = "none",
    use_gradient_checkpointing = True,
    random_state = 42,
)

# Instruction-tuning dataset (small slice for safety)
dataset = load_dataset(
    "tatsu-lab/alpaca",
    split="train[:2000]",
)

def format_prompt(example):
    return (
        "### Instruction:\n"
        f"{example['instruction']}\n\n"
        "### Response:\n"
        f"{example['output']}"
    )

dataset = dataset.map(
    lambda x: {"text": format_prompt(x)},
    remove_columns=dataset.column_names,
)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir = "./unsloth-output",
    per_device_train_batch_size = 2,
    gradient_accumulation_steps = 4,
    learning_rate = 2e-4,
    num_train_epochs = 1,
    fp16 = True,
    logging_steps = 10,
    save_strategy = "epoch",
    report_to = "none",
)

from trl import SFTTrainer

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = 512,
    args = training_args,
)

trainer.train()

model.save_pretrained("unsloth-lora-adapter")
tokenizer.save_pretrained("unsloth-lora-adapter")

FastLanguageModel.for_inference(model)

prompt = """### Instruction:
Explain LoRA in simple terms.

### Response:
"""

inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

with torch.no_grad():
    output = model.generate(
        **inputs,
        max_new_tokens=150,
        temperature=0.7,
    )

print(tokenizer.decode(output[0], skip_special_tokens=True))

# Fixed prompts used BOTH before and after fine-tuning
eval_prompts = [
    "Explain LoRA in simple terms.",
    "Write a Python function to reverse a string.",
    "What is gradient accumulation and why is it used?",
    "Summarize the concept of attention in transformers.",
    "Explain QLoRA vs LoRA.",
]


# SAME generation settings for fair comparison
GEN_KWARGS = dict(
    max_new_tokens = 200,
    temperature = 0.7,
    top_p = 0.9,
)

import torch

def run_evaluation(model, tokenizer, prompts, title):
    print(f"\n===== {title} =====\n")

    FastLanguageModel.for_inference(model)

    results = []

    for prompt in prompts:
        full_prompt = f"""### Instruction:
{prompt}

### Response:
"""
        inputs = tokenizer(full_prompt, return_tensors="pt").to("cuda")

        with torch.no_grad():
            outputs = model.generate(**inputs, **GEN_KWARGS)

        text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        results.append(text)

        print("PROMPT:", prompt)
        print("OUTPUT:")
        print(text)
        print("-" * 80)

    return results

#Before Finetuning
from unsloth import FastLanguageModel
import torch

# Load BASE model (no LoRA)
base_model, base_tokenizer = FastLanguageModel.from_pretrained(
    model_name = "mistralai/Mistral-7B-Instruct-v0.2",
    max_seq_length = 2048,
    dtype = torch.float16,
    load_in_4bit = True,
)

base_outputs = run_evaluation(
    base_model,
    base_tokenizer,
    eval_prompts,
    title="BEFORE FINE-TUNING (BASE MODEL)",
)

#After Finetuning
# Load fine-tuned model (base + LoRA adapters)
ft_model, ft_tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B-Instruct", # Changed to the model used for training
    max_seq_length = 2048,
    dtype = torch.float16,
    load_in_4bit = True,
)

ft_model.load_adapter("unsloth-lora-adapter")

ft_outputs = run_evaluation(
    ft_model,
    ft_tokenizer,
    eval_prompts,
    title="AFTER FINE-TUNING (LoRA MODEL)",
)

